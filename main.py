import streamlit as st
import os
import re
import pandas as pd  # [ì¶”ê°€] ì—‘ì…€ ë°ì´í„° ì²˜ë¦¬ìš©
import io            # [ì¶”ê°€] ì—‘ì…€ íŒŒì¼ ë©”ëª¨ë¦¬ ì €ì¥ìš©
import services.scraper_service as scraper
import services.ai_service as ai
import services.map_service as map_api
import services.image_service as image_gen 

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="AI íë ˆì´í„° Pro", page_icon="ğŸ¥", layout="centered")

st.title("ğŸ¥ ë³´ê³  ë“£ëŠ” AI ë§›ì§‘ íë ˆì´í„°")
st.caption("ìœ íŠœë¸Œ/ì¸ìŠ¤íƒ€ ì˜ìƒ ë§í¬ë¥¼ ë„£ìœ¼ë©´, AIê°€ **ê°„íŒì„ ì½ê³  ì†Œë¦¬ë¥¼ ë“¤ì–´ì„œ** ë§›ì§‘ì„ ì°¾ì•„ì¤ë‹ˆë‹¤!")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "analysis_result" not in st.session_state:
    st.session_state.analysis_result = None

# í…ìŠ¤íŠ¸ ì²­ì†Œ í•¨ìˆ˜ (ì¹´ë“œìš©)
def clean_text_for_card(text):
    if not text: return ""
    cleaned = re.sub(r'[^ê°€-í£a-zA-Z0-9\s\(\)\-\&]', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned

# ì…ë ¥ í¼
with st.form("input_form"):
    url = st.text_input("ë§í¬ ì…ë ¥ (ìœ íŠœë¸Œ, ì¸ìŠ¤íƒ€, ë¸”ë¡œê·¸)", placeholder="https://...")
    submitted = st.form_submit_button("ë¶„ì„ ì‹œì‘ ğŸš€", type="primary")

if submitted and url:
    is_video = "youtube.com" in url or "youtu.be" in url or "instagram.com" in url
    
    with st.status("ğŸ•µï¸ AIê°€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...", expanded=True) as status:
        
        # [A] ì˜ìƒ ì²˜ë¦¬
        if is_video:
            st.write("ğŸ“¥ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì„œë²„ ì„±ëŠ¥ í’€ê°€ë™!)")
            video_path, error = scraper.get_video_file(url)
            
            if error:
                status.update(label="âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨", state="error")
                st.error(error)
                st.stop()
            
            st.write("ğŸ§  Gemini 2.5 Proê°€ ì˜ìƒì„ ì‹œì²­ ì¤‘ì…ë‹ˆë‹¤... (ì‹œê°+ì²­ê° ë¶„ì„)")
            ai_result = ai.analyze_video(video_path)
            
            if os.path.exists(video_path):
                os.remove(video_path)

        # [B] í…ìŠ¤íŠ¸ ì²˜ë¦¬
        else:
            st.write("ğŸ“„ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...")
            raw_text = scraper.get_naver_blog_content(url) if "naver" in url else "í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€"
            st.write("ğŸ§  Gemini 2.5 Proê°€ í…ìŠ¤íŠ¸ë¥¼ ì½ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
            ai_result = ai.analyze_text(raw_text)

        # [ê³µí†µ] ì§€ë„ ê²€ìƒ‰
        places_data = []
        if ai_result.get("places"):
            st.write("ğŸ—ºï¸ êµ¬ê¸€ ì§€ë„ì—ì„œ ì •í™•í•œ ìœ„ì¹˜ ì°¾ëŠ” ì¤‘...")
            
            for place in ai_result["places"]:
                query = place.get("search_query", "ë§›ì§‘")
                map_info = map_api.search_place(query)
                review_summary = ""
                
                if map_info:
                    reviews = map_api.get_place_reviews(map_info['place_id'])
                    review_summary = ai.summarize_reviews(reviews)
                
                places_data.append({
                    "ai_info": place,
                    "map_info": map_info,
                    "review_summary": review_summary
                })
        
        st.session_state.analysis_result = {
            "summary": ai_result.get("summary"),
            "places_data": places_data,
            "url": url
        }
        status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete")

# --- ê²°ê³¼ ì¶œë ¥ ---
if st.session_state.analysis_result:
    res = st.session_state.analysis_result
    
    st.divider()
    
    # [ë¶€í™œ] ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì˜ì—­
    if res["places_data"]:
        excel_data = []
        for item in res["places_data"]:
            p_ai = item['ai_info']
            p_map = item['map_info']
            
            # ì—‘ì…€ì— ì €ì¥í•  ë°ì´í„° ì •ë¦¬
            name = p_map['name'] if p_map else p_ai.get('search_query')
            addr = p_map['address'] if p_map else "ì£¼ì†Œ ì •ë³´ ì—†ìŒ"
            rating = p_map['rating'] if p_map else 0.0
            link = map_api.get_map_link(p_map['place_id']) if p_map else ""
            
            excel_data.append({
                "ì‹ë‹¹ì´ë¦„": name,
                "í‰ì ": rating,
                "íŠ¹ì§•": p_ai.get('description', ''),
                "ë¦¬ë·°ìš”ì•½": item.get('review_summary', ''),
                "ì£¼ì†Œ": addr,
                "êµ¬ê¸€ë§µë§í¬": link
            })
            
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(excel_data)
        
        # ì—‘ì…€ íŒŒì¼ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ìƒì—ì„œ)
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='ë§›ì§‘ë¦¬ìŠ¤íŠ¸')
            
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
        st.download_button(
            label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ",
            data=buffer.getvalue(),
            file_name="AI_ë§›ì§‘ë¦¬ìŠ¤íŠ¸.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )

    st.subheader("ğŸ“ 3ì¤„ ìš”ì•½")
    if res.get("summary"):
        st.info(res["summary"])
    
    st.subheader("ğŸ“ ë°œê²¬ëœ ë§›ì§‘ ë¦¬ìŠ¤íŠ¸")
    
    if not res["places_data"]:
        st.write("ë°œê²¬ëœ ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    for item in res["places_data"]:
        p_ai = item['ai_info']
        p_map = item['map_info']
        review_summ = item.get('review_summary', '')
        
        # ì´ë¦„ ìš°ì„ ìˆœìœ„ (êµ¬ê¸€ë§µ > AI)
        if p_map and p_map.get('name'):
            original_name = p_map['name']
        elif p_ai.get('display_name'):
            original_name = p_ai['display_name']
        else:
            original_name = p_ai.get('search_query', 'ì•Œ ìˆ˜ ì—†ëŠ” ì‹ë‹¹')

        # ì¹´ë“œìš© ì´ë¦„ ì²­ì†Œ
        card_name_clean = clean_text_for_card(original_name)
        if not card_name_clean.strip():
            card_name_clean = clean_text_for_card(p_ai.get('display_name', 'Global Restaurant'))

        desc = p_ai.get('description', '')
        
        card_data = {
            "ì‹ë‹¹ì´ë¦„": card_name_clean,
            "í‰ì ": p_map['rating'] if p_map else 0.0,
            "íŠ¹ì§•": desc,
            "ë¦¬ë·°ìš”ì•½": review_summ,
            "ì§€ë„ë§í¬": map_api.get_map_link(p_map['place_id']) if p_map else "",
            "ì‚¬ì§„URL": p_map.get('photo_url') if p_map else None
        }

        with st.container():
            c1, c2 = st.columns([3, 2])
            
            with c1:
                st.markdown(f"### {original_name}")  
                st.write(f"ğŸ’¡ {desc}")
                if review_summ:
                    st.success(f"ğŸ—£ï¸ **í›„ê¸° ìš”ì•½:** {review_summ}")
                
                if p_map:
                    map_link = map_api.get_map_link(p_map['place_id'])
                    st.link_button("ğŸ—ºï¸ êµ¬ê¸€ ì§€ë„ ë³´ê¸°", map_link)
                    
            with c2:
                try:
                    img_path = image_gen.create_restaurant_card(card_data)
                    st.image(img_path, caption="ğŸ“¸ ì €ì¥í•´ì„œ ê³µìœ í•˜ì„¸ìš”!", use_container_width=True)
                except Exception as e:
                    st.error(f"ì¹´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
        
        st.markdown("---")
