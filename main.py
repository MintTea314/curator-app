import streamlit as st
import os
import re
import pandas as pd
import io
import services.scraper_service as scraper
import services.ai_service as ai
import services.map_service as map_api
import services.image_service as image_gen 

st.set_page_config(page_title="AI íë ˆì´í„° Pro", page_icon="ğŸ¥", layout="centered")

st.title("ğŸ¥ ë³´ê³  ë“£ëŠ” AI ë§›ì§‘ íë ˆì´í„°")
st.caption("ìœ íŠœë¸Œ ì‡¼ì¸ , ì¸ìŠ¤íƒ€ ë¦´ìŠ¤/ê²Œì‹œë¬¼ ë§í¬ë¥¼ ë„£ìœ¼ë©´ AIê°€ ë§›ì§‘ì„ ì°¾ì•„ì¤ë‹ˆë‹¤!")

if "analysis_result" not in st.session_state:
    st.session_state.analysis_result = None

def clean_text_for_card(text):
    if not text: return ""
    cleaned = re.sub(r'[^ê°€-í£a-zA-Z0-9\s\(\)\-\&]', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned

with st.form("input_form"):
    url = st.text_input("ë§í¬ ì…ë ¥ (Youtube, Instagram, Naver)", placeholder="https://...")
    submitted = st.form_submit_button("ë¶„ì„ ì‹œì‘ ğŸš€", type="primary")

if submitted and url:
    # ë§í¬ íƒ€ì… íŒë‹¨
    is_youtube = "youtube.com" in url or "youtu.be" in url
    is_instagram = "instagram.com" in url
    
    with st.status("ğŸ•µï¸ AIê°€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...", expanded=True) as status:
        
        ai_result = {"summary": "ë¶„ì„ ì‹¤íŒ¨", "places": []}
        
        # [A] ìœ íŠœë¸Œ ì²˜ë¦¬ (ì˜ìƒ)
        if is_youtube:
            st.write("ğŸ“¥ ìœ íŠœë¸Œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            video_path, error = scraper.get_video_file(url)
            if error:
                st.error(error)
                st.stop()
            
            st.write("ğŸ§  Geminiê°€ ìœ íŠœë¸Œ ì˜ìƒì„ ë¶„ì„ ì¤‘...")
            ai_result = ai.analyze_video(video_path)
            if os.path.exists(video_path): os.remove(video_path)

        # [B] ì¸ìŠ¤íƒ€ê·¸ë¨ ì²˜ë¦¬ (ë¦´ìŠ¤ or ê²Œì‹œë¬¼)
        elif is_instagram:
            st.write("ğŸ“¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  ê°€ì ¸ì˜¤ëŠ” ì¤‘ (Apify)...")
            # scraperê°€ 'video'ì¸ì§€ 'image'ì¸ì§€ ì•Œë ¤ì¤Œ
            content_type, content_path, error = scraper.get_instagram_content(url)
            
            if error:
                st.error(error)
                st.stop()
            
            if content_type == 'video':
                st.write("ğŸ¥ ë¦´ìŠ¤(ì˜ìƒ) ë¶„ì„ ì¤‘...")
                ai_result = ai.analyze_video(content_path)
                if os.path.exists(content_path): os.remove(content_path)
                
            elif content_type == 'image':
                st.write(f"ğŸ–¼ï¸ ì‚¬ì§„ ê²Œì‹œë¬¼({len(content_path)}ì¥) ë¶„ì„ ì¤‘...")
                ai_result = ai.analyze_images(content_path)
                # ì‚¬ìš©í•œ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
                for p in content_path:
                    if os.path.exists(p): os.remove(p)

        # [C] í…ìŠ¤íŠ¸ (ë¸”ë¡œê·¸ ë“±)
        else:
            st.write("ğŸ“„ í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            raw_text = scraper.get_naver_blog_content(url) if "naver" in url else "í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€"
            st.write("ğŸ§  í…ìŠ¤íŠ¸ ì½ëŠ” ì¤‘...")
            ai_result = ai.analyze_text(raw_text)

        # [ê³µí†µ] ì§€ë„ ê²€ìƒ‰ ë° ê²°ê³¼ ì •ë¦¬
        places_data = []
        if ai_result.get("places"):
            st.write("ğŸ—ºï¸ êµ¬ê¸€ ì§€ë„ì—ì„œ ìœ„ì¹˜ í™•ì¸ ì¤‘...")
            
            for place in ai_result["places"]:
                query = place.get("search_query", "ë§›ì§‘")
                map_info = map_api.search_place(query)
                review_summary = ""
                
                if map_info:
                    reviews = map_api.get_place_reviews(map_info['place_id'])
                    review_summary = ai.summarize_reviews(reviews)
                
                places_data.append({
                    "ai_info": place,
                    "map_info": map_info,
                    "review_summary": review_summary
                })
        
        st.session_state.analysis_result = {
            "summary": ai_result.get("summary"),
            "places_data": places_data,
            "url": url
        }
        status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete")

# --- ê²°ê³¼ í™”ë©´ ---
if st.session_state.analysis_result:
    res = st.session_state.analysis_result
    
    st.divider()
    st.subheader("ğŸ“ 3ì¤„ ìš”ì•½")
    if res.get("summary"):
        st.info(res["summary"])
    
    st.subheader("ğŸ“ ë°œê²¬ëœ ë§›ì§‘ ë¦¬ìŠ¤íŠ¸")
    
    if not res["places_data"]:
        st.write("ë°œê²¬ëœ ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    for item in res["places_data"]:
        p_ai = item['ai_info']
        p_map = item['map_info']
        review_summ = item.get('review_summary', '')
        
        if p_map and p_map.get('name'):
            original_name = p_map['name']
        elif p_ai.get('display_name'):
            original_name = p_ai['display_name']
        else:
            original_name = p_ai.get('search_query', 'ì•Œ ìˆ˜ ì—†ëŠ” ì‹ë‹¹')

        card_name_clean = clean_text_for_card(original_name)
        if not card_name_clean.strip():
            card_name_clean = clean_text_for_card(p_ai.get('display_name', 'Global Restaurant'))

        desc = p_ai.get('description', '')
        
        card_data = {
            "ì‹ë‹¹ì´ë¦„": card_name_clean,
            "í‰ì ": p_map['rating'] if p_map else 0.0,
            "íŠ¹ì§•": desc,
            "ë¦¬ë·°ìš”ì•½": review_summ,
            "ì§€ë„ë§í¬": map_api.get_map_link(p_map['place_id']) if p_map else "",
            "ì‚¬ì§„URL": p_map.get('photo_url') if p_map else None
        }

        with st.container():
            c1, c2 = st.columns([3, 2])
            with c1:
                st.markdown(f"### {original_name}")  
                st.write(f"ğŸ’¡ {desc}")
                if review_summ:
                    st.success(f"ğŸ—£ï¸ **í›„ê¸° ìš”ì•½:** {review_summ}")
                if p_map:
                    map_link = map_api.get_map_link(p_map['place_id'])
                    st.link_button("ğŸ—ºï¸ êµ¬ê¸€ ì§€ë„ ë³´ê¸°", map_link)
            with c2:
                try:
                    img_path = image_gen.create_restaurant_card(card_data)
                    st.image(img_path, caption="ğŸ“¸ ì €ì¥í•´ì„œ ê³µìœ í•˜ì„¸ìš”!", use_container_width=True)
                except Exception as e:
                    st.error(f"ì¹´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
        st.markdown("---")

    # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ìµœí•˜ë‹¨)
    if res["places_data"]:
        st.subheader("ğŸ“Š ë°ì´í„° ëª¨ì•„ë³´ê¸°")
        excel_data = []
        for item in res["places_data"]:
            p_ai = item['ai_info']
            p_map = item['map_info']
            excel_data.append({
                "ì‹ë‹¹ì´ë¦„": p_map['name'] if p_map else p_ai.get('search_query'),
                "í‰ì ": p_map['rating'] if p_map else 0.0,
                "íŠ¹ì§•": p_ai.get('description', ''),
                "ë¦¬ë·°ìš”ì•½": item.get('review_summary', ''),
                "ì£¼ì†Œ": p_map['address'] if p_map else "",
                "êµ¬ê¸€ë§µë§í¬": map_api.get_map_link(p_map['place_id']) if p_map else ""
            })
            
        df = pd.DataFrame(excel_data)
        st.dataframe(df, use_container_width=True)
        
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='ë§›ì§‘ë¦¬ìŠ¤íŠ¸')
            
        st.download_button(
            label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ",
            data=buffer.getvalue(),
            file_name="AI_ë§›ì§‘ë¦¬ìŠ¤íŠ¸.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
