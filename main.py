import streamlit as st
import os
import services.scraper_service as scraper
import services.ai_service as ai
import services.map_service as map_api
import services.image_service as image_gen # (ê¸°ì¡´ì— ë§Œë“  ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ ìœ ì§€)

st.set_page_config(page_title="AI íë ˆì´í„° Pro", page_icon="ğŸ¥", layout="centered")

st.title("ğŸ¥ ë³´ê³  ë“£ëŠ” AI ë§›ì§‘ íë ˆì´í„°")
st.caption("ìœ íŠœë¸Œ/ì¸ìŠ¤íƒ€ ì˜ìƒ ë§í¬ë¥¼ ë„£ìœ¼ë©´, AIê°€ **ê°„íŒì„ ì½ê³  ì†Œë¦¬ë¥¼ ë“¤ì–´ì„œ** ë§›ì§‘ì„ ì°¾ì•„ì¤ë‹ˆë‹¤!")

if "analysis_result" not in st.session_state:
    st.session_state.analysis_result = None

with st.form("input_form"):
    url = st.text_input("ë§í¬ ì…ë ¥ (ìœ íŠœë¸Œ, ì¸ìŠ¤íƒ€, ë¸”ë¡œê·¸)", placeholder="https://...")
    submitted = st.form_submit_button("ë¶„ì„ ì‹œì‘ ğŸš€", type="primary")

if submitted and url:
    # 1. ì˜ìƒì¸ì§€ í…ìŠ¤íŠ¸ì¸ì§€ íŒë‹¨
    is_video = "youtube.com" in url or "youtu.be" in url or "instagram.com" in url
    
    with st.status("ğŸ•µï¸ AIê°€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...", expanded=True) as status:
        
        # [A] ì˜ìƒ ì²˜ë¦¬ ëª¨ë“œ
        if is_video:
            st.write("ğŸ“¥ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì„œë²„ ì„±ëŠ¥ í’€ê°€ë™!)")
            video_path, error = scraper.get_video_file(url)
            
            if error:
                status.update(label="âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨", state="error")
                st.error(error)
                st.stop()
            
            st.write("ğŸ§  Gemini 2.0ì´ ì˜ìƒì„ ì‹œì²­ ì¤‘ì…ë‹ˆë‹¤... (ì‹œê°+ì²­ê° ë¶„ì„)")
            ai_result = ai.analyze_video(video_path)
            
            # ìš©ëŸ‰ ê´€ë¦¬ë¥¼ ìœ„í•´ íŒŒì¼ ì‚­ì œ
            if os.path.exists(video_path):
                os.remove(video_path)

        # [B] í…ìŠ¤íŠ¸(ë¸”ë¡œê·¸) ì²˜ë¦¬ ëª¨ë“œ
        else:
            st.write("ğŸ“„ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...")
            # (ê¸°ì¡´ ë¸”ë¡œê·¸ ë¡œì§ ê°„ì†Œí™” í˜¸ì¶œ)
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ë¼ë©´ scraper.get_naver_blog_content(url) ë“±ì„ í˜¸ì¶œ
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ì‹¬í”Œí•˜ê²Œ ì²˜ë¦¬í•œë‹¤ê³  ê°€ì •
            raw_text = scraper.get_naver_blog_content(url) if "naver" in url else "í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€"
            ai_result = ai.analyze_text(raw_text)

        # [ê³µí†µ] ì§€ë„ ì •ë³´ ê²€ìƒ‰
        places_data = []
        if ai_result.get("places"):
            st.write("ğŸ—ºï¸ êµ¬ê¸€ ì§€ë„ì—ì„œ ì •í™•í•œ ìœ„ì¹˜ ì°¾ëŠ” ì¤‘...")
            for place in ai_result["places"]:
                # AIê°€ ì°¾ì€ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
                map_info = map_api.search_place(place["search_query"])
                review_summary = ""
                
                if map_info:
                    # ë¦¬ë·° ê°€ì ¸ì™€ì„œ ìš”ì•½
                    reviews = map_api.get_place_reviews(map_info['place_id'])
                    review_summary = ai.summarize_reviews(reviews)
                
                places_data.append({
                    "ai_info": place,
                    "map_info": map_info,
                    "review_summary": review_summary
                })
        
        st.session_state.analysis_result = {
            "summary": ai_result.get("summary"),
            "places_data": places_data,
            "url": url
        }
        status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete")

# --- ê²°ê³¼ ì¶œë ¥ í™”ë©´ ---
if st.session_state.analysis_result:
    res = st.session_state.analysis_result
    
    st.divider()
    st.subheader("ğŸ“ 3ì¤„ ìš”ì•½")
    st.info(res["summary"])
    
    st.subheader("ğŸ“ ë°œê²¬ëœ ë§›ì§‘ ë¦¬ìŠ¤íŠ¸")
    
    for item in res["places_data"]:
        p_ai = item['ai_info']
        p_map = item['map_info']
        review_summ = item.get('review_summary', '')
        
        name = p_map['name'] if p_map else p_ai['search_query']
        desc = p_ai['description']
        
        # ì¹´ë“œ ë°ì´í„° êµ¬ì„±
        card_data = {
            "ì‹ë‹¹ì´ë¦„": name,
            "í‰ì ": p_map['rating'] if p_map else 0.0,
            "íŠ¹ì§•": desc,
            "ë¦¬ë·°ìš”ì•½": review_summ,
            "ì§€ë„ë§í¬": map_api.get_map_link(p_map['place_id']) if p_map else "",
            "ì‚¬ì§„URL": p_map.get('photo_url') if p_map else None
        }

        with st.container():
            c1, c2 = st.columns([3, 1])
            with c1:
                st.markdown(f"### {name}")
                st.write(f"ğŸ’¡ {desc}")
                if review_summ:
                    st.caption(f"ğŸ—£ï¸ **í›„ê¸° ìš”ì•½:** {review_summ}")
            with c2:
                # ì¹´ë“œ ì´ë¯¸ì§€ ìƒì„± (ìš°ë¦¬ê°€ í˜ë“¤ê²Œ ë§Œë“  Noto Sans ë²„ì „!)
                img = image_gen.create_restaurant_card(card_data)
                st.image(img, caption="ì €ì¥ìš© ì¹´ë“œ")
        
        st.markdown("---")
