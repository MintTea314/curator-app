import os
import uuid
import requests
from apify_client import ApifyClient
from dotenv import load_dotenv
from bs4 import BeautifulSoup
# [í™•ì‹¤í•˜ê²Œ ë³€ê²½] yt_dlp ë²„ë¦¬ê³  pytubefix ì‚¬ìš©
from pytubefix import YouTube 

load_dotenv()

def get_video_file(url):
    """
    URLì„ ë°›ì•„ ì˜ìƒ íŒŒì¼(mp4)ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜
    (ìœ íŠœë¸Œ: pytubefix OAuth TVì¸ì¦ / ì¸ìŠ¤íƒ€: Apify)
    """
    file_path = f"video_{uuid.uuid4()}.mp4"
    
    # [1] ì¸ìŠ¤íƒ€ê·¸ë¨
    if "instagram.com" in url:
        try:
            apify_token = os.getenv("APIFY_API_TOKEN")
            if not apify_token: return None, "APIFY_API_TOKEN ì—†ìŒ"
            
            client = ApifyClient(apify_token)
            run = client.actor("apify/instagram-reel-scraper").call(run_input={"urls": [url]})
            dataset_items = client.dataset(run["defaultDatasetId"]).list_items().items
            
            video_url = dataset_items[0].get("videoUrl") if dataset_items else None
            
            if video_url:
                with requests.get(video_url, stream=True) as r:
                    r.raise_for_status()
                    with open(file_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                return file_path, None
            return None, "ì¸ìŠ¤íƒ€ ì˜ìƒ ë§í¬ ì‹¤íŒ¨"
        except Exception as e:
            return None, f"ì¸ìŠ¤íƒ€ ì—ëŸ¬: {str(e)}"

    # [2] ìœ íŠœë¸Œ (Pytubefix - TV ì¸ì¦ ëª¨ë“œ)
    else:
        try:
            print(f"ğŸ“º Pytubefix(TVëª¨ë“œ)ë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„: {url}")
            
            # use_oauth=True, allow_oauth_cache=True í•„ìˆ˜!
            # ì•„ê¹Œ í„°ë¯¸ë„ì—ì„œ ë§Œë“  tokens.jsonì„ ìë™ìœ¼ë¡œ ì½ì–´ì„œ ì¸ì¦í•¨
            yt = YouTube(url, use_oauth=True, allow_oauth_cache=True)
            
            # ì‡¼ì¸ /ì¼ë°˜ ì˜ìƒ ëª¨ë‘ ëŒ€ì‘í•˜ê¸° ìœ„í•´ ìŠ¤íŠ¸ë¦¼ ê²€ìƒ‰ ë¡œì§ ê°•í™”
            stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()
            
            if not stream:
                # progressive(ì˜ìƒ+ìŒì„± í•©ë³¸)ê°€ ì—†ìœ¼ë©´ ì˜ìƒë§Œì´ë¼ë„ ê°€ì ¸ì˜¤ê¸° (ì‡¼ì¸  ëŒ€ë¹„)
                stream = yt.streams.filter(file_extension='mp4').order_by('resolution').desc().first()

            if stream:
                stream.download(filename=file_path)
                
                if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                    return file_path, None
                
            return None, "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì˜ìƒ ìŠ¤íŠ¸ë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"
            
        except Exception as e:
            if os.path.exists(file_path):
                os.remove(file_path)
            
            error_msg = str(e)
            # ì¸ì¦ í† í° ë¬¸ì œì¼ ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
            if "device" in error_msg or "code" in error_msg:
                return None, "ğŸš¨ ì¸ì¦ í† í° ë§Œë£Œ! í„°ë¯¸ë„ì—ì„œ 'python3 -c ...' ëª…ë ¹ì–´ë¡œ ë‹¤ì‹œ ì¸ì¦í•´ì£¼ì„¸ìš”."
            
            return None, f"ìœ íŠœë¸Œ ì—ëŸ¬: {error_msg}"

def get_naver_blog_content(url):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ (ê¸°ì¡´ ìœ ì§€)"""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36"}
        if "blog.naver.com" in url and "m.blog.naver.com" not in url:
            url = url.replace("blog.naver.com", "m.blog.naver.com")

        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        content_div = soup.find('div', class_='se-main-container')
        if not content_div: content_div = soup.find('div', class_='post_ct')
        return content_div.get_text(strip=True) if content_div else "ë³¸ë¬¸ ì—†ìŒ"
    except Exception as e:
        return f"ë¸”ë¡œê·¸ ì—ëŸ¬: {str(e)}"
