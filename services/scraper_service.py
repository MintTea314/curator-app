import os
import uuid
import yt_dlp
import requests
from apify_client import ApifyClient
from dotenv import load_dotenv
from bs4 import BeautifulSoup

load_dotenv()

def get_video_file(url):
    """
    URLì„ ë°›ì•„ ì˜ìƒ íŒŒì¼(mp4)ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜
    (ìœ íŠœë¸Œ/ì‡¼ì¸ /ì¸ìŠ¤íƒ€ ë¦´ìŠ¤ í†µí•© ì§€ì›)
    """
    # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
    file_path = f"video_{uuid.uuid4()}.mp4"
    
    # [1] ì¸ìŠ¤íƒ€ê·¸ë¨ (Apify ì‚¬ìš©)
    if "instagram.com" in url:
        try:
            apify_token = os.getenv("APIFY_API_TOKEN")
            if not apify_token:
                return None, "APIFY_API_TOKENì´ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤."
            
            # Apify í´ë¼ì´ì–¸íŠ¸ ì‹œì‘
            client = ApifyClient(apify_token)
            
            # ì¸ìŠ¤íƒ€ ë¦´ìŠ¤ ë‹¤ìš´ë¡œë” ì•¡í„° ì‹¤í–‰
            # (ë§Œì•½ ì‹¤í–‰ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë©´ íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ê³ ë ¤í•´ì•¼ í•¨)
            run = client.actor("apify/instagram-reel-scraper").call(run_input={"urls": [url]})
            
            # ê²°ê³¼ ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
            dataset_items = client.dataset(run["defaultDatasetId"]).list_items().items
            
            video_url = None
            if dataset_items:
                video_url = dataset_items[0].get("videoUrl")
            
            if video_url:
                # ì˜ìƒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                with requests.get(video_url, stream=True) as r:
                    r.raise_for_status()
                    with open(file_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                return file_path, None
            else:
                return None, "ì¸ìŠ¤íƒ€ ì˜ìƒ ë§í¬ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return None, f"ì¸ìŠ¤íƒ€ ë‹¤ìš´ë¡œë“œ ì—ëŸ¬: {str(e)}"

    # [2] ìœ íŠœë¸Œ & ì‡¼ì¸  (yt-dlp + ì•ˆë“œë¡œì´ë“œ ìš°íšŒ ëª¨ë“œ)
    else:
        # ì¿ í‚¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ì„œë²„ì˜ cookies.txt ì‚¬ìš©)
        cookie_file = 'cookies.txt'
        has_cookies = os.path.exists(cookie_file)
        
        ydl_opts = {
            'format': 'best[ext=mp4]/best',  # MP4 í¬ë§· ìµœìš°ì„ 
            'outtmpl': file_path,            # ì €ì¥ë  íŒŒì¼ëª…
            'quiet': True,                   # ì§€ì €ë¶„í•œ ë¡œê·¸ ìˆ¨ê¹€
            'no_warnings': True,
            'nocheckcertificate': True,      # SSL ì¸ì¦ì„œ ë¬´ì‹œ
            'ignoreerrors': True,            # ì—ëŸ¬ ë‚˜ë„ ë©ˆì¶”ì§€ ì•ŠìŒ
            'geo_bypass': True,              # êµ­ê°€ ì œí•œ ìš°íšŒ ì‹œë„
            
            # [í•µì‹¬ 1] ì¿ í‚¤ íŒŒì¼ ì ìš© (ìˆìœ¼ë©´ ì“°ê³ , ì—†ìœ¼ë©´ ì•ˆ ì”€)
            'cookiefile': cookie_file if has_cookies else None,
            
            # [í•µì‹¬ 2] ì˜¤ë¼í´ ì„œë²„ ì°¨ë‹¨ íšŒí”¼ìš© (ì•ˆë“œë¡œì´ë“œ ì•±ì¸ ì²™ ì†ì´ê¸°)
            'extractor_args': {
                'youtube': {
                    'player_client': ['android', 'web']
                }
            },
            
            # [í•µì‹¬ 3] í—¤ë” ì¡°ì‘ (ëª¨ë°”ì¼ ë¸Œë¼ìš°ì €ì¸ ì²™)
            'user_agent': 'Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
        }
        
        try:
            print(f"ğŸ¬ ë‹¤ìš´ë¡œë“œ ì‹œì‘... (Android ëª¨ë“œ, ì¿ í‚¤: {has_cookies})")
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([url])
            
            # íŒŒì¼ì´ ì§„ì§œ ìƒê²¼ëŠ”ì§€, ìš©ëŸ‰ì´ 0ì€ ì•„ë‹Œì§€ í™•ì¸
            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                return file_path, None
            else:
                return None, "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì°¨ë‹¨ë˜ì—ˆê±°ë‚˜ ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¿ í‚¤ë¥¼ ìµœì‹ ìœ¼ë¡œ êµì²´í•´ë³´ì„¸ìš”.)"
                
        except Exception as e:
            # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì“°ë ˆê¸° íŒŒì¼ì´ ë‚¨ì•˜ë‹¤ë©´ ì‚­ì œ
            if os.path.exists(file_path):
                os.remove(file_path)
            return None, f"ìœ íŠœë¸Œ ì—ëŸ¬: {str(e)}"

def get_naver_blog_content(url):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    """
    try:
        # ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ì ‘ì†í•´ì•¼ iframe ì—†ì´ ë³¸ë¬¸ì´ ë°”ë¡œ ë³´ì„
        headers = {
            "User-Agent": "Mozilla/5.0 (Linux; Android 10; SM-G960N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36"
        }
        # m.blog.naver.com ìœ¼ë¡œ ë³€í™˜ ì‹œë„
        if "blog.naver.com" in url and "m.blog.naver.com" not in url:
            url = url.replace("blog.naver.com", "m.blog.naver.com")

        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ (ë„¤ì´ë²„ ëª¨ë°”ì¼ ë¸”ë¡œê·¸ ê¸°ì¤€)
        content_div = soup.find('div', class_='se-main-container')
        if not content_div:
            content_div = soup.find('div', class_='post_ct') # êµ¬ë²„ì „
            
        if content_div:
            return content_div.get_text(strip=True)
        else:
            return "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¹„ê³µê°œ ê¸€ì´ê±°ë‚˜ êµ¬ì¡°ê°€ ë‹¤ë¦„)"
            
    except Exception as e:
        return f"ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì—ëŸ¬: {str(e)}"
