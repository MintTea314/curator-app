import os
import time
import requests
import re
from pytubefix import YouTube
from apify_client import ApifyClient
from dotenv import load_dotenv

load_dotenv()

# [ê¸°ì¡´] ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def get_video_file(url):
    """ìœ íŠœë¸Œ ì˜ìƒì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    try:
        yt = YouTube(url, use_oauth=True, allow_oauth_cache=True)
        print(f"ğŸ“¥ ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {yt.title}")
        
        # ì‡¼ì¸ ë‚˜ ì¼ë°˜ ì˜ìƒ ëª¨ë‘ ì²˜ë¦¬
        stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()
        if not stream:
            stream = yt.streams.filter(file_extension='mp4').order_by('resolution').desc().first()
            
        out_file = stream.download()
        
        # íŒŒì¼ëª… ë‹¨ìˆœí™” (ì˜¤ë¥˜ ë°©ì§€)
        base, ext = os.path.splitext(out_file)
        new_file = f"video_{int(time.time())}.mp4"
        os.rename(out_file, new_file)
        
        return new_file, None
    except Exception as e:
        return None, f"ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ ì—ëŸ¬: {str(e)}"

# [ì‹ ê·œ] ì¸ìŠ¤íƒ€ê·¸ë¨ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (Apify ì‚¬ìš©)
def get_instagram_content(url):
    """
    ì¸ìŠ¤íƒ€ ë§í¬ë¥¼ ë¶„ì„í•˜ì—¬ ì½˜í…ì¸ (ì˜ìƒ or ì´ë¯¸ì§€ë“¤)ë¥¼ ë‹¤ìš´ë¡œë“œí•¨
    ë°˜í™˜ê°’: (type, paths, error)
    type: 'video' ë˜ëŠ” 'image'
    paths: íŒŒì¼ ê²½ë¡œ(ë¬¸ìì—´) ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    api_token = os.getenv("APIFY_API_TOKEN")
    if not api_token:
        return None, None, "Apify API í† í°ì´ ì—†ìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

    client = ApifyClient(api_token)
    
    print(f"ğŸ“¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ë¶„ì„ ìš”ì²­ (Apify): {url}")

    # Apifyì˜ 'instagram-scraper' ì•¡í„° ì‚¬ìš©
    run_input = {
        "directUrls": [url],
        "resultsType": "details", # ìƒì„¸ ì •ë³´ í•„ìš”
        "searchLimit": 1,
    }

    try:
        # Actor ì‹¤í–‰
        run = client.actor("apify/instagram-scraper").call(run_input=run_input)
        
        # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        dataset_items = client.dataset(run["defaultDatasetId"]).list_items().items
        
        if not dataset_items:
            return None, None, "ì¸ìŠ¤íƒ€ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¹„ê³µê°œ ê³„ì •ì¼ ìˆ˜ ìˆìŒ)"
            
        item = dataset_items[0]
        
        # --- A. ë¦´ìŠ¤ (ì˜ìƒ) ì¸ ê²½ìš° ---
        # videoUrlì´ ì¡´ì¬í•˜ë©´ ì˜ìƒìœ¼ë¡œ ì·¨ê¸‰
        if item.get("videoUrl"):
            print("ğŸ¥ ë¦´ìŠ¤(ë™ì˜ìƒ) ê°ì§€ë¨")
            video_url = item["videoUrl"]
            
            # ì˜ìƒ ë‹¤ìš´ë¡œë“œ
            res = requests.get(video_url, stream=True)
            filename = f"insta_reel_{int(time.time())}.mp4"
            with open(filename, 'wb') as f:
                for chunk in res.iter_content(chunk_size=1024):
                    f.write(chunk)
            return "video", filename, None

        # --- B. ê²Œì‹œë¬¼ (ì‚¬ì§„) ì¸ ê²½ìš° ---
        # images ë¦¬ìŠ¤íŠ¸ë‚˜ displayUrl ì‚¬ìš©
        elif item.get("images") or item.get("displayUrl"):
            print("ğŸ–¼ï¸ ì‚¬ì§„ ê²Œì‹œë¬¼ ê°ì§€ë¨")
            image_urls = item.get("images", [])
            
            # ë§Œì•½ images ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¸ë„¤ì¼(displayUrl) í•˜ë‚˜ë¼ë„ ì”€
            if not image_urls and item.get("displayUrl"):
                image_urls = [item["displayUrl"]]
            
            # ìµœëŒ€ 5ì¥ê¹Œì§€ë§Œ ë‹¤ìš´ë¡œë“œ (AI í† í° ì ˆì•½)
            saved_files = []
            for i, img_url in enumerate(image_urls[:5]):
                try:
                    res = requests.get(img_url)
                    fname = f"insta_img_{int(time.time())}_{i}.jpg"
                    with open(fname, 'wb') as f:
                        f.write(res.content)
                    saved_files.append(fname)
                except:
                    continue
            
            if not saved_files:
                return None, None, "ì‚¬ì§„ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
            return "image", saved_files, None
            
        else:
            return None, None, "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸ìŠ¤íƒ€ í˜•ì‹ì…ë‹ˆë‹¤."

    except Exception as e:
        return None, None, f"Apify ì—ëŸ¬: {str(e)}"

# [í†µí•©] ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë“± í…ìŠ¤íŠ¸
def get_naver_blog_content(url):
    try:
        if "m.blog.naver.com" not in url:
            url = url.replace("blog.naver.com", "m.blog.naver.com")
        
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        content = soup.find('div', class_='se-main-container')
        if not content:
            content = soup.find('div', id='viewTypeSelector')
            
        return content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ"
    except Exception as e:
        return f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
