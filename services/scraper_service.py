import os
import yt_dlp
from apify_client import ApifyClient
from youtube_transcript_api import YouTubeTranscriptApi
from dotenv import load_dotenv

load_dotenv()

def get_youtube_data(video_url):
    """
    ìœ íŠœë¸Œ ë°ì´í„° ì¶”ì¶œ (ì•ˆì „ ëª¨ë“œ)
    """
    print(f"ğŸ¬ ë¶„ì„ ì‹œì‘: {video_url}")
    
    combined_text = []
    video_id = None
    
    # 1. yt-dlpë¡œ ë©”íƒ€ë°ì´í„° ë° ëŒ“ê¸€ ì¶”ì¶œ
    try:
        # [ìˆ˜ì •] ë³µì¡í•œ í•„í„°ë§ ì œê±°í•˜ê³  ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìš”ì²­
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'getcomments': True, # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° í•„ìˆ˜
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            print("â³ ìœ íŠœë¸Œ ì •ë³´(ëŒ“ê¸€ í¬í•¨) ë‹¤ìš´ë¡œë“œ ì¤‘...")
            info = ydl.extract_info(video_url, download=False)
            
            # ê¸°ë³¸ ì •ë³´
            title = info.get('title', 'ì œëª© ì—†ìŒ')
            description = info.get('description', 'ì„¤ëª… ì—†ìŒ')
            video_id = info.get('id')
            
            print(f"âœ… ì œëª© ì¶”ì¶œ ì™„ë£Œ: {title}")
            
            combined_text.append(f"== [ì˜ìƒ ì œëª©] ==\n{title}\n")
            combined_text.append(f"== [ì˜ìƒ ì„¤ëª…] ==\n{description}\n")
            
            # ëŒ“ê¸€ ì²˜ë¦¬ (íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ ì²˜ë¦¬)
            comments = info.get('comments', [])
            if comments:
                print(f"âœ… ëŒ“ê¸€ {len(comments)}ê°œ ë°œê²¬! ìƒìœ„ 10ê°œë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")
                top_comments = []
                # ê³ ì • ëŒ“ê¸€ì´ë‚˜ ì¸ê¸° ëŒ“ê¸€ì€ ë³´í†µ ì•ìª½ì— ìœ„ì¹˜í•¨
                for c in comments[:10]: 
                    author = c.get('author', 'Unknown')
                    text = c.get('text', '')
                    top_comments.append(f"- {author}: {text}")
                
                comments_text = "\n".join(top_comments)
                combined_text.append(f"== [ëŒ“ê¸€ ëª¨ìŒ] ==\n{comments_text}\n")
            else:
                print("âš ï¸ ëŒ“ê¸€ì„ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                combined_text.append("== [ëŒ“ê¸€] ==\n(ëŒ“ê¸€ ì—†ìŒ)\n")
            
    except Exception as e:
        print(f"âŒ yt-dlp ì—ëŸ¬: {e}")
        # ì—ëŸ¬ ë‚˜ë„ ìë§‰ì€ ì‹œë„í•˜ê¸° ìœ„í•´ ID ìˆ˜ë™ ì¶”ì¶œ
        if "v=" in video_url:
            video_id = video_url.split("v=")[1].split("&")[0]
        elif "youtu.be/" in video_url:
            video_id = video_url.split("youtu.be/")[1].split("?")[0]
        elif "shorts/" in video_url:
            video_id = video_url.split("shorts/")[1].split("?")[0]

    # 2. ìë§‰ ì¶”ì¶œ
    if video_id:
        try:
            yt = YouTubeTranscriptApi()
            transcript = yt.fetch(video_id, languages=['ko', 'en'])
            
            # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            script_text = ""
            for item in transcript:
                text = getattr(item, 'text', None) or item.get('text')
                if text:
                    script_text += text + " "
            
            combined_text.append(f"== [ì˜ìƒ ìë§‰] ==\n{script_text}")
            print("âœ… ìë§‰ ì¶”ì¶œ ì™„ë£Œ")
            
        except Exception:
            print("âš ï¸ ìë§‰ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            combined_text.append("\n(ìë§‰ ì—†ìŒ)")

    return "\n".join(combined_text), None

# ì¸ìŠ¤íƒ€ê·¸ë¨ ì½”ë“œëŠ” ë™ì¼
def get_instagram_data(insta_url):
    token = os.getenv("APIFY_API_TOKEN")
    if not token: return None, "Apify í† í° ì—†ìŒ"
    try:
        client = ApifyClient(token)
        run = client.actor("apify/instagram-scraper").call(run_input={
            "directUrls": [insta_url], "resultsType": "details", "searchLimit": 1
        })
        items = client.dataset(run["defaultDatasetId"]).list_items().items
        if items and items[0].get("caption"):
            return items[0].get("caption"), None
        return "[ë‚´ìš© ì—†ìŒ]", "ë¹„ê³µê°œ/ì°¨ë‹¨ë¨"
    except Exception as e:
        return None, str(e)