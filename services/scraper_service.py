import os
import uuid
import requests
from apify_client import ApifyClient
from dotenv import load_dotenv
from bs4 import BeautifulSoup
# [ë³€ê²½] yt_dlp ëŒ€ì‹  pytubefix ì‚¬ìš©
from pytubefix import YouTube 

load_dotenv()

def get_video_file(url):
    """
    URLì„ ë°›ì•„ ì˜ìƒ íŒŒì¼(mp4)ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜
    (ìœ íŠœë¸Œ: pytubefix OAuth ì¸ì¦ / ì¸ìŠ¤íƒ€: Apify)
    """
    # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
    file_path = f"video_{uuid.uuid4()}.mp4"
    
    # [1] ì¸ìŠ¤íƒ€ê·¸ë¨ (Apify ì‚¬ìš© - ê¸°ì¡´ ë™ì¼)
    if "instagram.com" in url:
        try:
            apify_token = os.getenv("APIFY_API_TOKEN")
            if not apify_token: return None, "APIFY í† í° ì—†ìŒ"
            
            client = ApifyClient(apify_token)
            run = client.actor("apify/instagram-reel-scraper").call(run_input={"urls": [url]})
            dataset_items = client.dataset(run["defaultDatasetId"]).list_items().items
            
            video_url = dataset_items[0].get("videoUrl") if dataset_items else None
            
            if video_url:
                with requests.get(video_url, stream=True) as r:
                    r.raise_for_status()
                    with open(file_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                return file_path, None
            return None, "ì¸ìŠ¤íƒ€ ë§í¬ ì°¾ê¸° ì‹¤íŒ¨"
        except Exception as e:
            return None, f"ì¸ìŠ¤íƒ€ ì—ëŸ¬: {str(e)}"

    # [2] ìœ íŠœë¸Œ (Pytubefix - TV ì¸ì¦ ëª¨ë“œ)
    else:
        try:
            print(f"ğŸ“º Pytubefix(TVëª¨ë“œ)ë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„: {url}")
            
            # use_oauth=True, allow_oauth_cache=True ì˜µì…˜ì´ í•µì‹¬!
            # ì•„ê¹Œ í„°ë¯¸ë„ì—ì„œ ë§Œë“  í† í° íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì½ì–´ì˜µë‹ˆë‹¤.
            yt = YouTube(url, use_oauth=True, allow_oauth_cache=True)
            
            # ê°€ì¥ í•´ìƒë„ ë†’ì€ mp4 ìŠ¤íŠ¸ë¦¼ ì„ íƒ
            stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()
            
            if not stream:
                # ì‡¼ì¸ ì˜ ê²½ìš° progressiveê°€ ì—†ì„ ìˆ˜ ìˆì–´ ë‹¤ì‹œ ê²€ìƒ‰
                stream = yt.streams.filter(file_extension='mp4').order_by('resolution').desc().first()

            if stream:
                stream.download(filename=file_path)
                
                if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                    return file_path, None
                
            return None, "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì˜ìƒ ìŠ¤íŠ¸ë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"
            
        except Exception as e:
            # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(file_path):
                os.remove(file_path)
            
            # í˜¹ì‹œ í† í° ë§Œë£Œ ì—ëŸ¬ì¸ì§€ í™•ì¸
            error_msg = str(e)
            if "device" in error_msg or "code" in error_msg:
                return None, "ì¸ì¦ í† í° ë§Œë£Œ! í„°ë¯¸ë„ì—ì„œ ë‹¤ì‹œ ì¸ì¦í•´ì£¼ì„¸ìš”."
            
            return None, f"ìœ íŠœë¸Œ ì—ëŸ¬: {error_msg}"

def get_naver_blog_content(url):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ (ê¸°ì¡´ ë™ì¼)"""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36"}
        if "blog.naver.com" in url and "m.blog.naver.com" not in url:
            url = url.replace("blog.naver.com", "m.blog.naver.com")

        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        content_div = soup.find('div', class_='se-main-container')
        if not content_div: content_div = soup.find('div', class_='post_ct')
            
        return content_div.get_text(strip=True) if content_div else "ë³¸ë¬¸ ì°¾ê¸° ì‹¤íŒ¨"
    except Exception as e:
        return f"ë¸”ë¡œê·¸ ì—ëŸ¬: {str(e)}"
